---
title: "Requirements"
description: "Use pre- and post-conditions to validate your LLM outputs meet specific requirements."
---

How can we ensure that the output from a language model is actually good?

Experienced generative programmers don’t leave this to chance. Instead, they define pre-conditions to check that inputs to the LLM are in the expected format, and post-conditions to verify that the outputs meet their quality standards.

Let’s say we want to ensure that a generated email includes a salutation and uses only lower-case letters. We can enforce these expectations by specifying requirements in the m.instruct call.

<CodeGroup>

```python Python
...
def write_email(m: mellea.MelleaSession, name: str, notes: str) -> str:
    email =  m.instruct(
        "Write an email to {{name}} using the notes following: {{notes}}.",
        requirements=[
            "The email should have a salutation",
            "Use only lower-case letters"
        ],
        user_dict={"name": name, "notes": notes},
    )
    return email.value

m = mellea.start_session()
print(write_email(m, "Olivia", "Olivia helped the lab over the last few weeks by organizing intern events, advertising the speaker series, and handling issues with snack delivery."))

```

</CodeGroup>

We just added two requirements to the instruction which will be added to the model request.
But we don't check yet if these requirements are satisfied. Let's add a **strategy** for
validating the requirements:

<CodeGroup>

```python Python
import mellea


def write_email(m: mellea.MelleaSession, name: str, notes: str) -> str:
  email_candidate = m.instruct(
    "Write an email to {{name}} using the notes following: {{notes}}.",
    requirements=[
      "The email should have a salutation",
      "Use only lower-case letters"
    ],
    strategy=m.strategy.smc(budget=5),
    # see SMC: https://arxiv.org/pdf/2306.03081
    user_dict={"name": name, "notes": notes},
  )
  if email_candidate.success():
    return email_candidate.as_string()
  else:
    return email_candidate.candidates[0].as_string()


m = mellea.start_session()
print(write_email(m, "Olivia",
                  "Olivia helped the lab over the last few weeks by organizing intern events, advertising the speaker series, and handling issues with snack delivery."))
```

</CodeGroup>
A couple of things happened here. First, we added a sampling `strategy` to the instruction.
This strategy (`m.strategy.smc()`) checks if all requirements are met.
If any requirement fails, then the sampling strategy will generate a new email with a stronger emphasis on the failing requirement(s).
This process will repeat until the `budget` on retries is consumed.

Even with retries, sampling might not generate results that fulfill all requirements (`email_candidate.success()==False`).
Mellea forces you to think about what it means for an LLM call to fail;
in this case, we handle the situation by simply reutrning the first sample as the final result.

### Validating Requirements

Now that we defined requirements and sampling we should have a
look into **how requirements are validated**. The default validation strategy is [LLM-as-a-judge](https://arxiv.org/abs/2306.05685).

Let's look on how we can customize requirement definitions:

<CodeGroup>

```python Python
from mellea.stdlib.requirement import req, check, simple_validate

requirements = [
    req("The email should have a salutation"),  # == r1
    req("Use only lower-case letters", validation_fn=simple_validate(lambda x: x.lower() == x)),  # == r2
    check("Do not mention purple elephants.")  # == r3
]
```

</CodeGroup>

Here, the first requirement (r1) has been given the highest priority of `MUST` which
means, that it must be fulfilled to lead to a positive result. In contrast, r2 should
be fulfilled - i.e. if two samples have both fulfilled r1 but only one of them r2,
then this one is the most preferred.

While r1 and r2 use LLM-as-a-judge for validation, the third requirement (r3) simply
uses a function that takes the output of a sampling step and returns a boolean
value indicating (un-)successful validation.

The forth requirement is a `check()`. Checks are only used for validation, not for generation.
Checks aim to avoid the "do not think about B" effect that often primes models (and humans)
to do the opposite and "think" about B.

> [!NOTE]
> LLMaJ is not presumtively robust. Whenever possible, implement requirement validation using plain old Python code. When a model is necessary, it can often be a good idea to train a **calibrated** model specifically for your validation problem. [Chapter 6](/core-concept/adapters) explains how to use Mellea's `m tune` subcommand to train your own LoRAs for requirement checking (and for other types of Mellea components as well).

### Instruct - Validate - Repair

Now, we bring it all together into a first generative program using the **instruct-validate-repair** pattern:

<CodeGroup>

```python Python

import mellea

def write_email(m: mellea.MelleaSession, name: str, notes: str) -> str:
email_candidate = m.instruct(
"Write an email to {{name}} using the notes following: {{notes}}.",
requirements=[
m.req("The email should have a salutation", priority=m.prio.MUST),

# == r1

m.req("Do not respond in all-caps", priority=m.prio.SHOULD), # == r2
m.req("Use only lower-case letters",
validator=lambda x: x.as_string().lower() == x.as_string()),

# == r3

m.check("Do not mention purple elephants.") # == r4
],
strategy=m.strategy.SMC(budget=5),
user_dict={"name": name, "notes": notes},
)
if email_candidate.success():
return email_candidate.as_string()
else:
return email_candidate.candidates[0].as_string()

m = mellea.start_session()
print(write_email(m, "Olivia",
"Olivia helped the lab over the last few weeks by organizing intern events, advertising the speaker series, and handling issues with snack delivery."))

```

</CodeGroup>

> [!NOTE]
> The `instruct()` method is a convenience function that creates and then generates from an `Instruction` Component, `req()` similarly wraps the `Requirement` Component, etc. [Chapter 2](/overview/quick-start) will takes us one level deeper into understanding what happens under the hood when you call `m.instruct()`.

### ModelOptions

Most LLM apis allow you to specify options to modify the request: temperature, max_tokens, seed, etc... Mellea supports specifying these options during backend initialization and when calling session-level functions with the `model_options` parameter.

Mellea supports many different types of inference engines (ollama, openai-compatible vllm, huggingface, etc.). These inference engines, which we call `Backend`s, provide different and sometimes inconsistent dict keysets for specifying model options. For the most common options among model providers, Mellea provides some engine-agnostic options, which can be used by typing [`ModelOption.<TAB>`](../mellea/backends/types.py) in your favorite IDE; for example, temperature can be specified as `{"{ModelOption.TEMPERATURE": 0}` and this will "just work" across all inference engines.

You can add any key-value pair supported by the backend to the `model_options` dictionary, and those options will be passed along to the inference engine \*even if a Mellea-specific `ModelOption.<KEY>` is defined for that option. This means you can safely copy over model option parameters from exiting codeb ases as-is:

<CodeGroup>

```python Python
m = MelleaSession(backend_name="openai")
m.instruct(..., model_options={
    "temperature": 0.5,
    "frequency_penalty": 1,
    "stop": "stop_string",
    "max_completion_tokens": 10,
})
```

</CodeGroup>

You can always update the model options of a given backend; however, Mellea offers a few additional approaches to changing the specified options.

1. **Specifying options during `m.*` calls**. Options specified here will update the model options previously specified for that call only. If you specify an already existing key (with either the `ModelOption.OPTION` version or the native name for that option for the given api), the value will be the one associated with the new key. If you specify the same key in different ways (ie `ModelOption.TEMPERATURE` and `temperature`), the `ModelOption.OPTION` key will take precedence.

<CodeGroup>

```python Python
orig_model_opts = {"k1": "v1", ModelOption.K2: "v2", "k4": "v4"} # options specified when creating the backend
new_model_opts = {ModelOption.K1: "v1.1", "k1": "v1.2", "k2": "v2.1", "k3": "v3"} # options specified with `m.*`

# final options passed to the provider

final_opts = {"k1": "v1.1", "k2": "v2.1", "k3": "v3", "k4": "v4"}

```

</CodeGroup>

2. **Pushing and popping model state**. Sessions offer the ability to push and pop model state. This means you can temporarily change the `model_options` for a series of calls by pushing a new set of `model_options` and then revert those changes with a pop.

### Conclusion

We have now worked up from a simple "Hello, World" example to our first generative programming design pattern: **Instruct - Validate - Reapir (IVR)**.

When LLMs work well, the software developer experiences the LLM as a sort of oracle that can handle most any input and produce a sufficiently desirable output. When LLMs do not work at all, the software developer experinces the LLM as a naive markov chain that produces junk. In both cases, the LLM is just sampling from a distribution.

The crux of generative programming is that most applications find themselves somewhere in-between these two extremes -- the LLM mostly works, enough to demo a tantilizing MVP. But failure modes are common enough and severe enough that complete automation is beyond the developer's grasp.

Traditional software deals with failure modes by carefully describing what can go wrong and then providing precise error handling logic. When working with LLMs, however, this approach suffers a Sysiphean curse. There is always one more failure mode, one more special case, one more new feature request. In the next chapter, we will explore how to build generative programs that are compositional and that grow gracefully.
