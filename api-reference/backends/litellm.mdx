---
title: mellea.backends.litellm
sidebar_label: Litellm
---

## Module: `mellea.backends.litellm`

A generic LiteLLM compatible backend that wraps around the openai python sdk.

### Classes

#### `class mellea.backends.litellm.LiteLLMBackend(model_id: str = 'ollama/' + str(model_ids.IBM_GRANITE_4_MICRO_3B.ollama_name), formatter: Formatter | None = None, base_url: str | None = 'http://localhost:11434', model_options: dict | None = None)`

A generic LiteLLM compatible backend.

##### Constructor

Initialize and OpenAI compatible backend. For any additional kwargs that you need to pass the the client, pass them as a part of **kwargs.

Note: If getting `Unclosed client session`, set `export DISABLE_AIOHTTP_TRANSPORT=True` in your environment. See: https://github.com/BerriAI/litellm/issues/13251.


## Arguments

* `model_id`: The LiteLLM model identifier. Make sure that all necessary credentials are in OS environment variables.
* `formatter`: A custom formatter based on backend.If None, defaults to TemplateFormatter
* `base_url`: Base url for LLM API. Defaults to None.
* `model_options`: Generation options to pass to the LLM. Defaults to None.

##### Methods

###### `mellea.backends.litellm.LiteLLMBackend.generate_from_context(action: Component | CBlock, ctx: Context, format: type[BaseModelSubclass] | None = None, model_options: dict | None = None, tool_calls: bool = False)`

See `generate_from_chat_context`.

-----

###### `mellea.backends.litellm.LiteLLMBackend._simplify_and_merge(model_options: dict[str, Any] | None)`

Simplifies model_options to use the Mellea specific ModelOption.Option and merges the backend's model_options with those passed into this call.

Rules:
- Within a model_options dict, existing keys take precedence. This means remapping to mellea specific keys will maintain the value of the mellea specific key if one already exists.
- When merging, the keys/values from the dictionary passed into this function take precedence.

Because this function simplifies and then merges, non-Mellea keys from the passed in model_options will replace
Mellea specific keys from the backend's model_options.


## Arguments

* `model_options`: the model_options for this call

a new dict

-----

###### `mellea.backends.litellm.LiteLLMBackend._make_backend_specific_and_remove(model_options: dict[str, Any])`

Maps specified Mellea specific keys to their backend specific version and removes any remaining Mellea keys.

Additionally, logs any params unknown to litellm and any params that are openai specific but not supported by this model/provider.


## Arguments

* `model_options`: the model_options for this call

a new dict

-----

###### `mellea.backends.litellm.LiteLLMBackend._generate_from_chat_context_standard(action: Component | CBlock, ctx: Context, format: type[BaseModelSubclass] | None = None, model_options: dict | None = None, tool_calls: bool = False)`

-----

###### `mellea.backends.litellm.LiteLLMBackend._extract_tools(action, format, model_opts, tool_calls, ctx)`

-----

###### `mellea.backends.litellm.LiteLLMBackend._generate_from_raw(actions: list[Component | CBlock], format: type[BaseModelSubclass] | None = None, model_options: dict | None = None, generate_logs: list[GenerateLog] | None = None)`

Generate using the completions api. Gives the input provided to the model without templating.

-----

###### `mellea.backends.litellm.LiteLLMBackend._extract_model_tool_requests(tools: dict[str, Callable], chat_response: litellm.ModelResponse)`

-----

---
